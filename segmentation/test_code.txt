model:
/workspace/ViT-Adapter/segmentation/mmseg_custom/models/segmentors/encoder_decoder_mask2former.py
/workspace/ViT-Adapter/segmentation/mmseg_custom/models/decode_heads/mask2former_head.py

img:
(1,3,512,512)

Backbone:
ipdb> x[0].shape
torch.Size([1, 768, 128, 128])
ipdb> x[1].shape
torch.Size([1, 768, 64, 64])
ipdb> x[2].shape
torch.Size([1, 768, 32, 32])
ipdb> x[3].shape
torch.Size([1, 768, 16, 16])

Tranformer:
ipdb> cls_pred_list[0].shape
torch.Size([1, 100, 172])
ipdb> query_feat.shape
torch.Size([100, 1, 256])
ipdb> mask_features.shape
torch.Size([1, 256, 128, 128])

ipdb> cls_score.shape
torch.Size([1, 100, 172]) [batch_size, num_queries, cls_out_channels]
ipdb> mask_pred.shape
torch.Size([1, 100, 128, 128]) [batch_size, num_queries, h, w]


CUDA_VISIBLE_DEVICES=0 python test.py configs/coco_stuff10k/mask2former_beit_adapter_base_512_40k_cocostuff10k_ss.py ./pretrained/mask2former_beit_adapter_base_512_40k_cocostuff10k.pth.tar --eval mIoU 